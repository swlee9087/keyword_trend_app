# streamlit_naver_api.py

import requests
import pandas as pd
import streamlit as st
import json
import math
from datetime import datetime, timedelta
from env_loader import load_naver_credentials
from log_util import logger

def chunk_keywords(keywords, chunk_size=5, group_size=20):
    """í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ Naver APIì— ë§ê²Œ ê·¸ë£¹ë³„ë¡œ ë‚˜ëˆ„ê¸°"""
    chunks = []
    for i in range(0, len(keywords), group_size):
        group_keywords = keywords[i:i+group_size]
        group_name = group_keywords[0]
        chunks.append({"groupName": group_name, "keywords": group_keywords})
    return [chunks[i:i+chunk_size] for i in range(0, len(chunks), chunk_size)]

def fetch_naver_trends(keyword_groups_batch, start_date, end_date, client_id, client_secret):
    url = "https://openapi.naver.com/v1/datalab/search"

    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
        "Content-Type": "application/json"
    }

    data = {
        "startDate": start_date,
        "endDate": end_date,
        "timeUnit": "date",
        "keywordGroups": keyword_groups_batch
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        logger.log(f">>>>>> Naver API í˜¸ì¶œ ì„±ê³µ: status_code={response.status_code}")
        return response.json()
    else:
        error_msg = f">>> Naver API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}, {response.text}"
        logger.log(error_msg)
        raise Exception(error_msg)

def collect_trend_data(keywords, days=7):
    st.subheader("ğŸ” ë„¤ì´ë²„ ê²€ìƒ‰ì–´ íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘")
    try:
        client_id, client_secret = load_naver_credentials()
        # st.caption("âœ… .envì—ì„œ ì¸ì¦ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        logger.log(">>>>>> ì¸ì¦ ì •ë³´ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        st.error(str(e))
        logger.log(f">>> ì¸ì¦ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

    today = datetime.today().date()
    start_date = (today - timedelta(days=days)).isoformat()
    end_date = today.isoformat()
    logger.log(f">>>>>> ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    logger.log(f">>>>>> ì´ í‚¤ì›Œë“œ ìˆ˜: {len(keywords)}")

    keyword_batches = chunk_keywords(keywords)
    logger.log(f">>>>>> í‚¤ì›Œë“œ ê·¸ë£¹ ë°°ì¹˜ ìˆ˜: {len(keyword_batches)}")
    
    all_data = []
    with st.spinner("ë„¤ì´ë²„ APIë¡œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
        for batch in keyword_batches:
            logger.log(f">>>>>> [ë°°ì¹˜ {batch}/{len(keyword_batches)}] í˜¸ì¶œí•  ê·¸ë£¹: {[g['groupName'] for g in batch]}")
            try:
                result = fetch_naver_trends(batch, start_date, end_date, client_id, client_secret)
                for group in result['results']:
                    df = pd.DataFrame(group['data'])
                    df['group'] = group['title']
                    all_data.append(df)
                    logger.log(f">>>>>>ê·¸ë£¹ '{group['title']}' ë°ì´í„° ìˆ˜ì§‘: {len(group['data'])}ê±´")
            except Exception as e:
                st.warning(f"âŒ ì¼ë¶€ ê·¸ë£¹ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                logger.log(f">>> [ë°°ì¹˜ {batch_idx}] ì¼ë¶€ ê·¸ë£¹ ì‹¤íŒ¨: {e}")

    if all_data:
        full_df = pd.concat(all_data, ignore_index=True)
        full_df['period'] = pd.to_datetime(full_df['period'])
        st.success("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        logger.log(f">>>>>> ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(full_df)}í–‰")
        return full_df
    else:
        st.error("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        logger.log(">>> ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼: 0í–‰")
        return None
